{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "bert_approach.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IOp-EUUVPYc"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAy88prr5SG"
      },
      "source": [
        "# %%capture\n",
        "# !pip install wandb -qqq\n",
        "# !pip install transformers -qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpKsiDtKr7Dr"
      },
      "source": [
        "# !wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR1TPOxDWWQW"
      },
      "source": [
        "# !cp '/content/drive/My Drive/hw/all_train.csv' 'all_train.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:29.706516Z",
          "start_time": "2021-03-28T19:28:29.700589Z"
        },
        "id": "fajNnUNzUtwV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import string\n",
        "import nltk\n",
        "import ast\n",
        "import re\n",
        "import wandb\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "from torch import nn\n",
        "import transformers\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:30.439486Z",
          "start_time": "2021-03-28T19:28:30.433326Z"
        },
        "id": "b6adVCUGUtwo"
      },
      "source": [
        "# disabling warnings about max length of tokens\n",
        "import logging\n",
        "import re\n",
        "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
        "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
        "    for name in logging.root.manager.loggerDict:\n",
        "        if re.match(prefix_re, name):\n",
        "            logging.getLogger(name).setLevel(level)\n",
        "set_global_logging_level(logging.ERROR, [\"transformers\", \"nlp\", \"torch\", \"tensorflow\", \"tensorboard\", \"wandb\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:32.703230Z",
          "start_time": "2021-03-28T19:28:32.700169Z"
        },
        "id": "weBQoTE1Utwq"
      },
      "source": [
        "RESOURCES_PATH = os.curdir\n",
        "DRIVE_PATH = '/content/drive/My Drive/hw/'\n",
        "BATCH_SIZE = 12\n",
        "N_EPOCH = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:33.961417Z",
          "start_time": "2021-03-28T19:28:33.432584Z"
        },
        "id": "byKb2l_mUtwr"
      },
      "source": [
        "df = pd.read_csv(os.path.join(RESOURCES_PATH, 'all_train.csv'))\n",
        "# df = df.sample(frac=1.).reset_index(drop=True)\n",
        "# df_train, df_test = df[:9000], df[9000:10000]\n",
        "df_train, df_test = df[:24000], df[24000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:36.905845Z",
          "start_time": "2021-03-28T19:28:36.900481Z"
        },
        "id": "wHQ-a7AHUtws"
      },
      "source": [
        "class ClassificationBERTModel(nn.Module):\n",
        "    def __init__(self, big=False, dropout=False):\n",
        "        super(ClassificationBERTModel, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        if dropout:\n",
        "            self.linear = nn.Sequential(nn.Linear(768, 256),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(0.4),\n",
        "                                        nn.Linear(256, 64),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(0.4),\n",
        "                                        nn.Linear(64, 3))\n",
        "        elif big:\n",
        "            self.linear = nn.Sequential(nn.Linear(768, 256),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(256, 64),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(64, 3))\n",
        "        else:\n",
        "            self.linear = nn.Sequential(nn.Linear(768, 3))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        a, out = self.bert(input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        return self.linear(out)\n",
        "    \n",
        "    def freeze_bert(self, freeze):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "    def save(self, name, all=True):\n",
        "        if all:\n",
        "           torch.save(self.state_dict(), DRIVE_PATH + 'bert_' + name)\n",
        "        else:\n",
        "           torch.save(self.linear.state_dict(), DRIVE_PATH + 'bert_linear_' + name)\n",
        "      \n",
        "    @staticmethod\n",
        "    def load(name, all=True, big=False, dropout=False):\n",
        "        model = ClassificationBERTModel(big, dropout)\n",
        "        if all:\n",
        "            model.load_state_dict(torch.load(DRIVE_PATH + 'bert_' + name))\n",
        "        else:\n",
        "            model.linear.load_state_dict(torch.load(DRIVE_PATH + 'bert_linear_' + name))\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:37.395451Z",
          "start_time": "2021-03-28T19:28:37.389902Z"
        },
        "id": "jesCCB3fUtwv"
      },
      "source": [
        "def to_one_hot(i, n):\n",
        "    res = [0] * n\n",
        "    res[i] = 1\n",
        "    return np.array(res)\n",
        "\n",
        "def transform(df):\n",
        "    tokenized = df.Text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,\n",
        "                                                          max_length=512)))\n",
        "    max_len = 0\n",
        "    for i in tokenized.values:\n",
        "        if len(i) > max_len:\n",
        "            max_len = len(i)\n",
        "\n",
        "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "    y_c = df['FinalScore1'] if 'FinalScore1' in df.columns else df['Score']\n",
        "    y = np.array([(x + 1) for x in y_c.values])\n",
        "    attention_mask = np.where(padded != 0, 1, 0)\n",
        "    \n",
        "    input_ids = torch.tensor(padded)  \n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    y = torch.tensor(y)\n",
        "    \n",
        "    return input_ids, attention_mask, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:29:59.801644Z",
          "start_time": "2021-03-28T19:29:59.797419Z"
        },
        "id": "ibwZ8K_LUtwx"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def evaluate(model, df, criterion, use_tqdm=True):\n",
        "    predicts = []\n",
        "    targets = []\n",
        "    losses = []\n",
        "    itt = df.groupby(np.arange(len(df)) // BATCH_SIZE)\n",
        "    if use_tqdm:\n",
        "        itt = tqdm(itt)\n",
        "    for _, batch in itt:\n",
        "        input_ids, attention_mask, y = transform(batch)\n",
        "        input_ids, attention_mask, y = input_ids.to(device), attention_mask.to(device), y.to(device)\n",
        "        predict = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(predict, y)\n",
        "        predict = torch.max(predict, 1)[1].detach().tolist()\n",
        "        targets += y.detach().tolist()\n",
        "        predicts += predict\n",
        "        losses.append(loss.detach().item())\n",
        "    return f1_score(targets, predicts, average='macro'), accuracy_score(targets, predicts), sum(losses) / len(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:30:10.691034Z",
          "start_time": "2021-03-28T19:30:05.819603Z"
        },
        "id": "3FMZeP6jUtwz"
      },
      "source": [
        "# Load pretrained model/tokenizer\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = ClassificationBERTModel(dropout=True).to(device)\n",
        "model.freeze_bert(False)\n",
        "wandb.init(project=\"bert-fine-tune\", name='dropout', reinit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:24.840392Z",
          "start_time": "2021-03-28T19:27:55.686933Z"
        },
        "id": "LjaTwgXPUtw1"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00002)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, N_EPOCH + 1):\n",
        "    df_train = df_train.sample(frac=1.).reset_index(drop=True)\n",
        "    batches = tqdm(df_train.groupby(np.arange(len(df_train)) // BATCH_SIZE), desc='EPOCH 1 Mean loss: NaN')\n",
        "    losses = []\n",
        "    for i, (_, batch) in enumerate(batches):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids, attention_mask, y = transform(batch)\n",
        "        input_ids, attention_mask, y = input_ids.to(device), attention_mask.to(device), y.to(device)\n",
        "        predict = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(predict, y)\n",
        "\n",
        "        loss.backward()\n",
        "        losses.append(loss.detach().item())\n",
        "        optimizer.step()\n",
        "        if (i + 1) % 50 == 0:\n",
        "            batches.set_description(f'EPOCH {epoch} loss: {round(sum(losses) / len(losses), 4)}')\n",
        "            wandb.log({\"current_loss\": sum(losses) / len(losses)})\n",
        "            losses = []\n",
        "        if (i + 1) % 500 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                f1_train, _, loss_train = evaluate(model, df_train.sample(n=1000).reset_index(drop=True), criterion, use_tqdm=False)\n",
        "\n",
        "                f1_test, _, loss_test = evaluate(model, df_test.sample(n=1000).reset_index(drop=True), criterion, use_tqdm=False)\n",
        "                print(f'EPOCH {epoch} {i + 1} train loss: {round(loss_train, 4)} test loss: {round(loss_test, 4)}'\n",
        "                      f' train f1: {round(f1_train, 4)} test f1: {round(f1_test, 4)}')\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        'train_loss': loss_train,\n",
        "                        'test_loss': loss_test,\n",
        "                        'train_f1': f1_train,\n",
        "                        'test_f1': f1_test\n",
        "                    }\n",
        "                )\n",
        "                model.save('dropout_' + str(epoch) + '_' + str(i + 1))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        f1, accuracy, loss = evaluate(model, df_test, criterion)\n",
        "        print(f'FULL Test # {epoch}: F1 macro = {f1} Accuracy = {accuracy}')\n",
        "        wandb.log(\n",
        "                    {\n",
        "                        'full_test_f1': f1\n",
        "                    }\n",
        "                )\n",
        "    model.save('dropout_' + str(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKu0k1HMp8NN"
      },
      "source": [
        "def test(name):\n",
        "  model = ClassificationBERTModel(big)\n",
        "  model.linear.load_state_dict(torch.load(DRIVE_PATH + name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSZ6rS760RRM"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "evaluate(ClassificationBERTModel.load('just_bert_3_1000', big=True).to(device), df_test, nn.CrossEntropyLoss(), use_tqdm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww-zZoV394jE"
      },
      "source": [
        "def apply(model, df, use_tqdm=True):\n",
        "    predicts = []\n",
        "    itt = df.groupby(np.arange(len(df)) // BATCH_SIZE)\n",
        "    if use_tqdm:\n",
        "        itt = tqdm(itt)\n",
        "    for _, batch in itt:\n",
        "        input_ids, attention_mask, y = transform(batch)\n",
        "        input_ids, attention_mask, y = input_ids.to(device), attention_mask.to(device), y.to(device)\n",
        "        predict = model(input_ids, attention_mask=attention_mask)\n",
        "        predict = torch.max(predict, 1)[1].detach().tolist()\n",
        "        predict = [(x - 1) for x in predict]\n",
        "        predicts += predict\n",
        "    return np.array(predicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pgMrPuXYBoq"
      },
      "source": [
        "df_super = pd.read_csv('/content/drive/My Drive/hw/test_final.csv')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "df_super['Score'] = pd.Series(apply(ClassificationBERTModel.load('just_bert_3_1000', big=True).to(device), df_super, use_tqdm=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zTK4rE7YhYv"
      },
      "source": [
        "df_super['Score'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZzJvk_xa2ty"
      },
      "source": [
        "!mkdir '/content/drive/My Drive/hw/results'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZxaffT_Zi5V"
      },
      "source": [
        "df_super.to_csv('/content/drive/My Drive/hw/results/test_final.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9bIdwVya7JW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}