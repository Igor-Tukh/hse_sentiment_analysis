{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "bert_approach_without_cutting.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IOp-EUUVPYc"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAy88prr5SG"
      },
      "source": [
        "# %%capture\n",
        "# !pip install wandb -qqq\n",
        "# !pip install transformers -qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpKsiDtKr7Dr"
      },
      "source": [
        "# !wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR1TPOxDWWQW"
      },
      "source": [
        "# !cp '/content/drive/My Drive/hw/all_train.csv' 'all_train.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:29.706516Z",
          "start_time": "2021-03-28T19:28:29.700589Z"
        },
        "id": "fajNnUNzUtwV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import string\n",
        "import nltk\n",
        "import ast\n",
        "import re\n",
        "import wandb\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "from torch import nn\n",
        "import transformers\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:30.439486Z",
          "start_time": "2021-03-28T19:28:30.433326Z"
        },
        "id": "b6adVCUGUtwo"
      },
      "source": [
        "# disabling warnings about max length of tokens\n",
        "import logging\n",
        "import re\n",
        "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
        "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
        "    for name in logging.root.manager.loggerDict:\n",
        "        if re.match(prefix_re, name):\n",
        "            logging.getLogger(name).setLevel(level)\n",
        "set_global_logging_level(logging.ERROR, [\"transformers\", \"nlp\", \"torch\", \"tensorflow\", \"tensorboard\", \"wandb\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:32.703230Z",
          "start_time": "2021-03-28T19:28:32.700169Z"
        },
        "id": "weBQoTE1Utwq"
      },
      "source": [
        "RESOURCES_PATH = os.curdir\n",
        "DRIVE_PATH = '/content/drive/My Drive/hw/'\n",
        "BATCH_SIZE = 8\n",
        "N_EPOCH = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:33.961417Z",
          "start_time": "2021-03-28T19:28:33.432584Z"
        },
        "id": "byKb2l_mUtwr"
      },
      "source": [
        "df = pd.read_csv(os.path.join(RESOURCES_PATH, 'all_train.csv'))\n",
        "# df = df.sample(frac=1.).reset_index(drop=True)\n",
        "# df_train, df_test = df[:9000], df[9000:10000]\n",
        "df_train, df_test = df[:24000], df[24000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:36.905845Z",
          "start_time": "2021-03-28T19:28:36.900481Z"
        },
        "id": "wHQ-a7AHUtws"
      },
      "source": [
        "class ClassificationBERTModel(nn.Module):\n",
        "    def __init__(self, big=False, dropout=False):\n",
        "        super(ClassificationBERTModel, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        if dropout:\n",
        "            self.linear = nn.Sequential(nn.Linear(768, 256),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(0.4),\n",
        "                                        nn.Linear(256, 64),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(0.4),\n",
        "                                        nn.Linear(64, 3))\n",
        "        elif big:\n",
        "            self.linear = nn.Sequential(nn.Linear(768, 256),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(256, 64),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(64, 3))\n",
        "        else:\n",
        "            self.linear = nn.Sequential(nn.Linear(768, 3))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        a, out = self.bert(input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        return self.linear(out)\n",
        "    \n",
        "    def freeze_bert(self, freeze):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "    def save(self, name, all=True):\n",
        "        if all:\n",
        "           torch.save(self.state_dict(), DRIVE_PATH + 'bert_' + name)\n",
        "        else:\n",
        "           torch.save(self.linear.state_dict(), DRIVE_PATH + 'bert_linear_' + name)\n",
        "      \n",
        "    @staticmethod\n",
        "    def load(name, all=True, big=False, dropout=False):\n",
        "        model = ClassificationBERTModel(big, dropout)\n",
        "        if all:\n",
        "            model.load_state_dict(torch.load(DRIVE_PATH + 'bert_' + name))\n",
        "        else:\n",
        "            model.linear.load_state_dict(torch.load(DRIVE_PATH + 'bert_linear_' + name))\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:37.395451Z",
          "start_time": "2021-03-28T19:28:37.389902Z"
        },
        "id": "jesCCB3fUtwv"
      },
      "source": [
        "def to_one_hot(i, n):\n",
        "    res = [0] * n\n",
        "    res[i] = 1\n",
        "    return np.array(res)\n",
        "\n",
        "def transform(df):\n",
        "    tokenized = []\n",
        "    y = []\n",
        "    ids = []\n",
        "    texts = df.Text.values\n",
        "    ys = df.FinalScore1.values\n",
        "    for i in range(len(df)):\n",
        "        x = texts[i]\n",
        "        tokens = tokenizer.encode(x, add_special_tokens=True)\n",
        "        if len(tokens) > 512:\n",
        "            tokens = tokens[1:-1]\n",
        "            for j in range(len(tokens) // 510 + 1):\n",
        "                part = tokens[j * 510: (j + 1) * 510]\n",
        "                if len(part) < 100:\n",
        "                    break\n",
        "                tokenized.append([tokenizer.cls_token_id] + part + [tokenizer.sep_token_id])\n",
        "                y.append(ys[i] + 1)\n",
        "                ids.append(i)\n",
        "        else:\n",
        "            tokenized.append(tokens)\n",
        "            y.append(ys[i] + 1)\n",
        "            ids.append(i)\n",
        "    max_len = 0\n",
        "    for i in tokenized:\n",
        "        if len(i) > max_len:\n",
        "            max_len = len(i)\n",
        "\n",
        "    padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
        "    y = np.array(y)\n",
        "    attention_mask = np.where(padded != 0, 1, 0)\n",
        "    \n",
        "    input_ids = torch.tensor(padded)  \n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    y = torch.tensor(y)\n",
        "    assert len(input_ids) == len(y) and len(attention_mask) == len(y) and len(ids) == len(y)\n",
        "    return input_ids, attention_mask, y, ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:29:59.801644Z",
          "start_time": "2021-03-28T19:29:59.797419Z"
        },
        "id": "ibwZ8K_LUtwx"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def evaluate(model, df, criterion, use_tqdm=True):\n",
        "    predicts = []\n",
        "    targets = []\n",
        "    losses = []\n",
        "    itt = df.groupby(np.arange(len(df)) // BATCH_SIZE)\n",
        "    if use_tqdm:\n",
        "        itt = tqdm(itt)\n",
        "    for _, batch in itt:\n",
        "        input_ids1, attention_mask1, y1, ids1 = transform(batch)\n",
        "        res = {}\n",
        "        for j in range(len(input_ids1) // BATCH_SIZE + 1):\n",
        "            input_ids, attention_mask, y, ids = input_ids1[j * BATCH_SIZE: (j + 1) * BATCH_SIZE].to(device), attention_mask1[j * BATCH_SIZE: (j + 1) * BATCH_SIZE].to(device), y1[j * BATCH_SIZE: (j + 1) * BATCH_SIZE].to(device), ids1[j * BATCH_SIZE: (j + 1) * BATCH_SIZE]\n",
        "            if len(input_ids) == 0:\n",
        "              break\n",
        "            predict = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(predict, y)\n",
        "            predict = predict.detach().tolist()\n",
        "            y = y.detach().tolist()\n",
        "            for i, p, t in zip(ids, predict, y):\n",
        "                res[i] = res.get(i, {'p': [], 't': []})\n",
        "                res[i]['p'].append(p)\n",
        "                res[i]['t'].append(t)\n",
        "            losses.append(loss.detach().item())\n",
        "        \n",
        "        for r in res.values():\n",
        "            p, t = r['p'], r['t']\n",
        "            t = t[0]\n",
        "            p = np.argmax(np.array(p).sum(axis=0))\n",
        "            predicts.append(p)\n",
        "            targets.append(t)\n",
        "    return f1_score(targets, predicts, average='macro'), accuracy_score(targets, predicts), sum(losses) / len(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ-rV-1YLUMY"
      },
      "source": [
        "# tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "# model = ClassificationBERTModel(big=True).to(device)\n",
        "# model.freeze_bert(False)\n",
        "# model.eval()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# with torch.no_grad():\n",
        "#     f1_test, _, loss_test = evaluate(model, df_test.sample(n=1000).reset_index(drop=True), criterion, use_tqdm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:30:10.691034Z",
          "start_time": "2021-03-28T19:30:05.819603Z"
        },
        "id": "3FMZeP6jUtwz"
      },
      "source": [
        "# Load pretrained model/tokenizer\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = ClassificationBERTModel(big=True).to(device)\n",
        "model.freeze_bert(False)\n",
        "\n",
        "wandb.init(project=\"bert-fine-tune\", name='no-cut', reinit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-28T19:28:24.840392Z",
          "start_time": "2021-03-28T19:27:55.686933Z"
        },
        "id": "LjaTwgXPUtw1"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00002)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, N_EPOCH + 1):\n",
        "    df_train = df_train.sample(frac=1.).reset_index(drop=True)\n",
        "    batches = tqdm(df_train.groupby(np.arange(len(df_train)) // BATCH_SIZE), desc='EPOCH 1 Mean loss: NaN')\n",
        "    losses = []\n",
        "    for i, (_, batch) in enumerate(batches):\n",
        "        model.train()\n",
        "\n",
        "        input_ids1, attention_mask1, y1, _ = transform(batch)\n",
        "\n",
        "        for j in range(len(input_ids1) // BATCH_SIZE + 1):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, y = input_ids1[j * BATCH_SIZE: (j + 1) * BATCH_SIZE].to(device), attention_mask1[j * BATCH_SIZE: (j + 1) * BATCH_SIZE].to(device), y1[j * BATCH_SIZE: (j + 1) * BATCH_SIZE].to(device)\n",
        "            if len(input_ids) == 0:\n",
        "              break\n",
        "            predict = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(predict, y)\n",
        "\n",
        "            loss.backward()\n",
        "            losses.append(loss.detach().item())\n",
        "            optimizer.step()\n",
        "        if (i + 1) % 75 == 0:\n",
        "            batches.set_description(f'EPOCH {epoch} loss: {round(sum(losses) / len(losses), 4)}')\n",
        "            wandb.log({\"current_loss\": sum(losses) / len(losses)})\n",
        "            losses = []\n",
        "        if (i + 1) % 750 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                f1_train, _, loss_train = evaluate(model, df_train.sample(n=1000).reset_index(drop=True), criterion, use_tqdm=False)\n",
        "\n",
        "                f1_test, _, loss_test = evaluate(model, df_test.sample(n=1000).reset_index(drop=True), criterion, use_tqdm=False)\n",
        "                print(f'EPOCH {epoch} {i + 1} train loss: {round(loss_train, 4)} test loss: {round(loss_test, 4)}'\n",
        "                      f' train f1: {round(f1_train, 4)} test f1: {round(f1_test, 4)}')\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        'train_loss': loss_train,\n",
        "                        'test_loss': loss_test,\n",
        "                        'train_f1': f1_train,\n",
        "                        'test_f1': f1_test\n",
        "                    }\n",
        "                )\n",
        "                model.save('nocut_' + str(epoch) + '_' + str(i + 1))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        f1, accuracy, loss = evaluate(model, df_test, criterion)\n",
        "        print(f'FULL Test # {epoch}: F1 macro = {f1} Accuracy = {accuracy}')\n",
        "        wandb.log(\n",
        "                    {\n",
        "                        'full_test_f1': f1\n",
        "                    }\n",
        "                )\n",
        "    model.save('nocut_' + str(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6OPlPIs9z7e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}